{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33449e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         User-ID         ISBN  Rating\n",
      "0         276725   034545104X       0\n",
      "1         276726   0155061224       5\n",
      "2         276727   0446520802       0\n",
      "3         276729   052165615X       3\n",
      "4         276729   0521795028       6\n",
      "...          ...          ...     ...\n",
      "1149775   276704   1563526298       9\n",
      "1149776   276706   0679447156       0\n",
      "1149777   276709   0515107662      10\n",
      "1149778   276721   0590442449      10\n",
      "1149779   276723  05162443314       8\n",
      "\n",
      "[1149780 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings_df = pd.read_csv('Ratings.csv', sep=';', dtype={'User-ID': int, 'ISBN': str, 'Rating': int})\n",
    "print(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319f0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(ratings_df['User-ID'].unique())}\n",
    "book_mapping = {isbn: idx for idx, isbn in enumerate(ratings_df['ISBN'].unique())}\n",
    "\n",
    "ratings_df['User-ID-Mapped'] = ratings_df['User-ID'].map(user_mapping)\n",
    "ratings_df['ISBN-Mapped'] = ratings_df['ISBN'].map(book_mapping)\n",
    "\n",
    "user_ids = ratings_df['User-ID-Mapped']\n",
    "book_ids = ratings_df['ISBN-Mapped']\n",
    "ratings = ratings_df['Rating']\n",
    "sparse_matrix = coo_matrix((ratings, (user_ids, book_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836c7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User and book mappings have been saved with the specified titles to 'user_mapping.csv' and 'book_mapping.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the user and book mappings to DataFrames with specified column titles\n",
    "user_mapping_df = pd.DataFrame(list(user_mapping.items()), columns=['User-ID', 'Mapped-ID'])\n",
    "book_mapping_df = pd.DataFrame(list(book_mapping.items()), columns=['ISBN', 'Mapped-ID'])\n",
    "\n",
    "# Save the mappings to CSV files with the specified titles\n",
    "user_mapping_df.to_csv('user_mapping.csv', index=False)\n",
    "book_mapping_df.to_csv('book_mapping.csv', index=False)\n",
    "\n",
    "print(\"User and book mappings have been saved with the specified titles to 'user_mapping.csv' and 'book_mapping.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0118864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix saved without labels in LibSVM format: user_book_matrix.libsvm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "dummy_labels = [0] * sparse_matrix.shape[0]\n",
    "\n",
    "temp_file_path = 'temp_user_book_matrix.libsvm'\n",
    "dump_svmlight_file(sparse_matrix, dummy_labels, temp_file_path, zero_based=True)\n",
    "\n",
    "output_file_path = 'user_book_matrix.libsvm'\n",
    "\n",
    "with open(temp_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        stripped_line = ' '.join(line.split()[1:])\n",
    "        if stripped_line.strip():\n",
    "            outfile.write(stripped_line + '\\n')\n",
    "\n",
    "print(f\"Sparse matrix saved without labels in LibSVM format: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c63afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "def calculate_sparse_cosine_similarity(team_32_vector, team_32_matrix):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between a single vector and all rows of a sparse matrix.\n",
    "\n",
    "    Parameters:\n",
    "        team_32_vector (csr_matrix): A sparse vector (row) for comparison.\n",
    "        team_32_matrix (csr_matrix): Sparse matrix where each row represents a different entity.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of cosine similarity scores for each row in the matrix.\n",
    "    \"\"\"\n",
    "    team_32_similarity = team_32_matrix.dot(team_32_vector.T).toarray().ravel()\n",
    "    team_32_vector_norm = np.sqrt(team_32_vector.multiply(team_32_vector).sum())\n",
    "    team_32_row_norms = np.sqrt(team_32_matrix.power(2).sum(axis=1)).A1\n",
    "    team_32_scores = team_32_similarity / (team_32_vector_norm * team_32_row_norms + 1e-10)\n",
    "    return team_32_scores\n",
    "\n",
    "def recommend_global_top_books(team_32_sparse_matrix, team_32_count=10):\n",
    "    \"\"\"\n",
    "    Recommend globally popular books based on average ratings across all users.\n",
    "\n",
    "    Parameters:\n",
    "        team_32_sparse_matrix (csr_matrix): Sparse matrix of user-item ratings.\n",
    "        team_32_count (int): Number of top books to return.\n",
    "\n",
    "    Returns:\n",
    "        list: Top books as tuples [(book_id, average_rating), ...].\n",
    "    \"\"\"\n",
    "    team_32_avg_scores = team_32_sparse_matrix.mean(axis=0).A1\n",
    "    team_32_top_indices = np.argsort(team_32_avg_scores)[-team_32_count:][::-1]\n",
    "    team_32_global_books = [(idx, team_32_avg_scores[idx]) for idx in team_32_top_indices]\n",
    "    return team_32_global_books\n",
    "\n",
    "def generate_book_recommendations(team_32_sparse_matrix, team_32_similar_users, team_32_target_user, team_32_top_k=10):\n",
    "    \"\"\"\n",
    "    Generate book recommendations for a user using their top-K similar users.\n",
    "\n",
    "    Parameters:\n",
    "        team_32_sparse_matrix (csr_matrix): Sparse user-item ratings matrix.\n",
    "        team_32_similar_users (list): List of tuples [(user_id, similarity_score), ...].\n",
    "        team_32_target_user (int): ID of the user to recommend books for.\n",
    "        team_32_top_k (int): Number of similar users to consider.\n",
    "\n",
    "    Returns:\n",
    "        list: Recommended books as tuples [(book_id, predicted_score), ...].\n",
    "    \"\"\"\n",
    "    team_32_user_books = set(team_32_sparse_matrix.getrow(team_32_target_user).nonzero()[1])\n",
    "    team_32_all_books = set()\n",
    "    \n",
    "    for team_32_user_id, _ in team_32_similar_users:\n",
    "        team_32_all_books.update(team_32_sparse_matrix.getrow(team_32_user_id).nonzero()[1])\n",
    "\n",
    "    team_32_recommend_pool = team_32_all_books - team_32_user_books\n",
    "    team_32_predicted_scores = {}\n",
    "\n",
    "    for team_32_book in team_32_recommend_pool:\n",
    "        team_32_total_score = 0\n",
    "        team_32_weight_sum = 0\n",
    "        for team_32_user_id, team_32_similarity in team_32_similar_users:\n",
    "            team_32_book_rating = team_32_sparse_matrix[team_32_user_id, team_32_book]\n",
    "            if team_32_book_rating > 0:\n",
    "                team_32_total_score += team_32_similarity * team_32_book_rating\n",
    "                team_32_weight_sum += team_32_similarity\n",
    "\n",
    "        if team_32_weight_sum == 0:\n",
    "            team_32_predicted_scores[team_32_book] = team_32_sparse_matrix[:, team_32_book].mean()\n",
    "        else:\n",
    "            team_32_predicted_scores[team_32_book] = team_32_total_score / team_32_weight_sum\n",
    "\n",
    "    team_32_personalized_books = sorted(team_32_predicted_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    if len(team_32_personalized_books) < 5:\n",
    "        team_32_needed_books = 5 - len(team_32_personalized_books)\n",
    "        team_32_global_books = recommend_global_top_books(\n",
    "            team_32_sparse_matrix, \n",
    "            team_32_count=team_32_needed_books + len(team_32_personalized_books)\n",
    "        )\n",
    "        team_32_existing_books = {book_id for book_id, _ in team_32_personalized_books}\n",
    "        team_32_additional_books = [\n",
    "            (book_id, score) for book_id, score in team_32_global_books \n",
    "            if book_id not in team_32_existing_books\n",
    "        ]\n",
    "        team_32_additional_books = team_32_additional_books[:team_32_needed_books]\n",
    "        team_32_personalized_books.extend(team_32_additional_books)\n",
    "\n",
    "    return team_32_personalized_books\n",
    "\n",
    "def find_top_similar_users_sparse(team_32_matrix, team_32_user_id, team_32_top_k=10):\n",
    "    \"\"\"\n",
    "    Identify the top-K most similar users for a specified user based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "        team_32_matrix (csr_matrix): Sparse matrix of user-item ratings.\n",
    "        team_32_user_id (int): The user ID for whom similarity is calculated.\n",
    "        team_32_top_k (int): The number of similar users to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: Top-K similar users as tuples [(user_id, similarity_score), ...].\n",
    "    \"\"\"\n",
    "    team_32_user_vector = team_32_matrix.getrow(team_32_user_id)\n",
    "    team_32_similarities = calculate_sparse_cosine_similarity(team_32_user_vector, team_32_matrix)\n",
    "    team_32_similarities[team_32_user_id] = -1\n",
    "    team_32_top_indices = np.argsort(team_32_similarities)[-team_32_top_k:][::-1]\n",
    "    team_32_similar_users = [(index, team_32_similarities[index]) for index in team_32_top_indices]\n",
    "    return team_32_similar_users\n",
    "\n",
    "def process_user_chunk_and_save(\n",
    "    team_32_data_file, team_32_user_map_file, team_32_book_map_file, \n",
    "    team_32_titles_file, team_32_result_file, team_32_user_group, team_32_num_similar=10):\n",
    "    \"\"\"\n",
    "    Process a chunk of users and save their book recommendations to a file, including titles.\n",
    "\n",
    "    Parameters:\n",
    "        team_32_data_file (str): Path to the LIBSVM file.\n",
    "        team_32_user_map_file (str): Path to the user mapping file.\n",
    "        team_32_book_map_file (str): Path to the book mapping file.\n",
    "        team_32_titles_file (str): Path to the file containing book titles.\n",
    "        team_32_result_file (str): Path to save the recommendations.\n",
    "        team_32_user_group (list): List of user IDs to process.\n",
    "        team_32_num_similar (int): Number of similar users to consider for recommendations.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    team_32_matrix, _ = load_svmlight_file(team_32_data_file)\n",
    "    team_32_user_map = pd.read_csv(team_32_user_map_file)\n",
    "    team_32_book_map = pd.read_csv(team_32_book_map_file)\n",
    "    team_32_titles = pd.read_csv(team_32_titles_file, delimiter=';')\n",
    "    team_32_output = []\n",
    "\n",
    "    for team_32_user in team_32_user_group:\n",
    "        team_32_user_row = team_32_user_map[team_32_user_map['Mapped-ID'] == team_32_user]\n",
    "        if team_32_user_row.empty:\n",
    "            continue\n",
    "\n",
    "        team_32_resolved_user = team_32_user_row['User-ID'].values[0]\n",
    "        team_32_similar_users = find_top_similar_users_sparse(team_32_matrix, team_32_user, team_32_num_similar)\n",
    "        team_32_recommendations = generate_book_recommendations(team_32_matrix, team_32_similar_users, team_32_user)\n",
    "\n",
    "        for team_32_book_id, team_32_score in team_32_recommendations:\n",
    "            team_32_book_row = team_32_book_map[team_32_book_map['Mapped-ID'] == team_32_book_id]\n",
    "            if team_32_book_row.empty:\n",
    "                team_32_isbn = \"Unknown ISBN\"\n",
    "            else:\n",
    "                team_32_isbn = team_32_book_row['ISBN'].values[0]\n",
    "\n",
    "            team_32_title_row = team_32_titles[team_32_titles['ISBN'] == team_32_isbn]\n",
    "            if team_32_title_row.empty:\n",
    "                team_32_book_title = \"Unknown Title\"\n",
    "            else:\n",
    "                team_32_book_title = team_32_title_row['Title'].values[0]\n",
    "\n",
    "            team_32_output.append({\n",
    "                'User_ID': team_32_resolved_user,\n",
    "                'Book_ID': team_32_isbn,\n",
    "                'Book_Title': team_32_book_title,\n",
    "                'Recommendation_Score': team_32_score\n",
    "            })\n",
    "\n",
    "    team_32_df = pd.DataFrame(team_32_output)\n",
    "    if not os.path.isfile(team_32_result_file):\n",
    "        team_32_df.to_csv(team_32_result_file, index=False)\n",
    "    else:\n",
    "        team_32_df.to_csv(team_32_result_file, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5501df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBSVM file created successfully: output.libsvm\n"
     ]
    }
   ],
   "source": [
    "input_file = \"user_book_matrix.libsvm\"\n",
    "output_file = \"output.libsvm\"\n",
    "\n",
    "with open(input_file, \"r\") as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for index, line in enumerate(lines):\n",
    "        features = line.strip()\n",
    "        if features:\n",
    "            label = index\n",
    "            libsvm_line = f\"{label} {features}\"\n",
    "            outfile.write(libsvm_line + \"\\n\")\n",
    "\n",
    "print(f\"LIBSVM file created successfully: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1723a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"output.libsvm\"\n",
    "user_mapping_file = \"user_mapping.csv\"\n",
    "book_mapping_file = \"book_mapping.csv\"\n",
    "titles_file = \"Books.csv\"\n",
    "result_file = \"final_recommendations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee85ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "\n",
    "sparse_matrix, _ = load_svmlight_file(data_file)\n",
    "\n",
    "chunk_size = 100\n",
    "total_users = sparse_matrix.shape[0]\n",
    "\n",
    "user_chunks = [range(i, min(i + chunk_size, total_users)) for i in range(0, total_users, chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509c7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 720/1053 with 100 users...\n",
      "Processing chunk 721/1053 with 100 users...\n",
      "Processing chunk 722/1053 with 100 users...\n",
      "Processing chunk 723/1053 with 100 users...\n",
      "Processing chunk 724/1053 with 100 users...\n",
      "Processing chunk 725/1053 with 100 users...\n",
      "Processing chunk 726/1053 with 100 users...\n",
      "Processing chunk 727/1053 with 100 users...\n",
      "Processing chunk 728/1053 with 100 users...\n",
      "Processing chunk 729/1053 with 100 users...\n",
      "Processing chunk 730/1053 with 100 users...\n",
      "Processing chunk 731/1053 with 100 users...\n",
      "Processing chunk 732/1053 with 100 users...\n",
      "Processing chunk 733/1053 with 100 users...\n",
      "Processing chunk 734/1053 with 100 users...\n",
      "Processing chunk 735/1053 with 100 users...\n",
      "Processing chunk 736/1053 with 100 users...\n",
      "Processing chunk 737/1053 with 100 users...\n",
      "Processing chunk 738/1053 with 100 users...\n",
      "Processing chunk 739/1053 with 100 users...\n",
      "Processing chunk 740/1053 with 100 users...\n",
      "Processing chunk 741/1053 with 100 users...\n",
      "Processing chunk 742/1053 with 100 users...\n",
      "Processing chunk 743/1053 with 100 users...\n",
      "Processing chunk 744/1053 with 100 users...\n",
      "Processing chunk 745/1053 with 100 users...\n",
      "Processing chunk 746/1053 with 100 users...\n",
      "Processing chunk 747/1053 with 100 users...\n",
      "Processing chunk 748/1053 with 100 users...\n",
      "Processing chunk 749/1053 with 100 users...\n",
      "Processing chunk 750/1053 with 100 users...\n",
      "Processing chunk 751/1053 with 100 users...\n",
      "Processing chunk 752/1053 with 100 users...\n",
      "Processing chunk 753/1053 with 100 users...\n",
      "Processing chunk 754/1053 with 100 users...\n",
      "Processing chunk 755/1053 with 100 users...\n",
      "Processing chunk 756/1053 with 100 users...\n",
      "Processing chunk 757/1053 with 100 users...\n",
      "Processing chunk 758/1053 with 100 users...\n",
      "Processing chunk 759/1053 with 100 users...\n",
      "Processing chunk 760/1053 with 100 users...\n",
      "Processing chunk 761/1053 with 100 users...\n",
      "Processing chunk 762/1053 with 100 users...\n",
      "Processing chunk 763/1053 with 100 users...\n",
      "Processing chunk 764/1053 with 100 users...\n",
      "Processing chunk 765/1053 with 100 users...\n",
      "Processing chunk 766/1053 with 100 users...\n",
      "Processing chunk 767/1053 with 100 users...\n",
      "Processing chunk 768/1053 with 100 users...\n",
      "Processing chunk 769/1053 with 100 users...\n",
      "Processing chunk 770/1053 with 100 users...\n",
      "Processing chunk 771/1053 with 100 users...\n",
      "Processing chunk 772/1053 with 100 users...\n",
      "Processing chunk 773/1053 with 100 users...\n",
      "Processing chunk 774/1053 with 100 users...\n",
      "Processing chunk 775/1053 with 100 users...\n",
      "Processing chunk 776/1053 with 100 users...\n",
      "Processing chunk 777/1053 with 100 users...\n",
      "Processing chunk 778/1053 with 100 users...\n",
      "Processing chunk 779/1053 with 100 users...\n",
      "Processing chunk 780/1053 with 100 users...\n",
      "Processing chunk 781/1053 with 100 users...\n",
      "Processing chunk 782/1053 with 100 users...\n",
      "Processing chunk 783/1053 with 100 users...\n",
      "Processing chunk 784/1053 with 100 users...\n",
      "Processing chunk 785/1053 with 100 users...\n",
      "Processing chunk 786/1053 with 100 users...\n",
      "Processing chunk 787/1053 with 100 users...\n",
      "Processing chunk 788/1053 with 100 users...\n",
      "Processing chunk 789/1053 with 100 users...\n",
      "Processing chunk 790/1053 with 100 users...\n",
      "Processing chunk 791/1053 with 100 users...\n",
      "Processing chunk 792/1053 with 100 users...\n",
      "Processing chunk 793/1053 with 100 users...\n",
      "Processing chunk 794/1053 with 100 users...\n",
      "Processing chunk 795/1053 with 100 users...\n",
      "Processing chunk 796/1053 with 100 users...\n",
      "Processing chunk 797/1053 with 100 users...\n",
      "Processing chunk 798/1053 with 100 users...\n",
      "Processing chunk 799/1053 with 100 users...\n",
      "Processing chunk 800/1053 with 100 users...\n",
      "Processing chunk 801/1053 with 100 users...\n",
      "Processing chunk 802/1053 with 100 users...\n",
      "Processing chunk 803/1053 with 100 users...\n",
      "Processing chunk 804/1053 with 100 users...\n",
      "Processing chunk 805/1053 with 100 users...\n",
      "Processing chunk 806/1053 with 100 users...\n",
      "Processing chunk 807/1053 with 100 users...\n",
      "Processing chunk 808/1053 with 100 users...\n",
      "Processing chunk 809/1053 with 100 users...\n",
      "Processing chunk 810/1053 with 100 users...\n",
      "Processing chunk 811/1053 with 100 users...\n",
      "Processing chunk 812/1053 with 100 users...\n",
      "Processing chunk 813/1053 with 100 users...\n",
      "Processing chunk 814/1053 with 100 users...\n",
      "Processing chunk 815/1053 with 100 users...\n",
      "Processing chunk 816/1053 with 100 users...\n",
      "Processing chunk 817/1053 with 100 users...\n",
      "Processing chunk 818/1053 with 100 users...\n",
      "Processing chunk 819/1053 with 100 users...\n",
      "Processing chunk 820/1053 with 100 users...\n",
      "Processing chunk 821/1053 with 100 users...\n",
      "Processing chunk 822/1053 with 100 users...\n",
      "Processing chunk 823/1053 with 100 users...\n",
      "Processing chunk 824/1053 with 100 users...\n",
      "Processing chunk 825/1053 with 100 users...\n",
      "Processing chunk 826/1053 with 100 users...\n",
      "Processing chunk 827/1053 with 100 users...\n",
      "Processing chunk 828/1053 with 100 users...\n",
      "Processing chunk 829/1053 with 100 users...\n",
      "Processing chunk 830/1053 with 100 users...\n",
      "Processing chunk 831/1053 with 100 users...\n",
      "Processing chunk 832/1053 with 100 users...\n",
      "Processing chunk 833/1053 with 100 users...\n",
      "Processing chunk 834/1053 with 100 users...\n",
      "Processing chunk 835/1053 with 100 users...\n",
      "Processing chunk 836/1053 with 100 users...\n",
      "Processing chunk 837/1053 with 100 users...\n",
      "Processing chunk 838/1053 with 100 users...\n",
      "Processing chunk 839/1053 with 100 users...\n",
      "Processing chunk 840/1053 with 100 users...\n",
      "Processing chunk 841/1053 with 100 users...\n",
      "Processing chunk 842/1053 with 100 users...\n",
      "Processing chunk 843/1053 with 100 users...\n",
      "Processing chunk 844/1053 with 100 users...\n",
      "Processing chunk 845/1053 with 100 users...\n",
      "Processing chunk 846/1053 with 100 users...\n",
      "Processing chunk 847/1053 with 100 users...\n",
      "Processing chunk 848/1053 with 100 users...\n",
      "Processing chunk 849/1053 with 100 users...\n",
      "Processing chunk 850/1053 with 100 users...\n",
      "Processing chunk 851/1053 with 100 users...\n",
      "Processing chunk 852/1053 with 100 users...\n",
      "Processing chunk 853/1053 with 100 users...\n",
      "Processing chunk 854/1053 with 100 users...\n",
      "Processing chunk 855/1053 with 100 users...\n",
      "Processing chunk 856/1053 with 100 users...\n",
      "Error processing chunk 856: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 857/1053 with 100 users...\n",
      "Error processing chunk 857: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 858/1053 with 100 users...\n",
      "Error processing chunk 858: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 859/1053 with 100 users...\n",
      "Error processing chunk 859: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 860/1053 with 100 users...\n",
      "Error processing chunk 860: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 861/1053 with 100 users...\n",
      "Error processing chunk 861: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 862/1053 with 100 users...\n",
      "Error processing chunk 862: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 863/1053 with 100 users...\n",
      "Error processing chunk 863: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 864/1053 with 100 users...\n",
      "Error processing chunk 864: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 865/1053 with 100 users...\n",
      "Error processing chunk 865: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 866/1053 with 100 users...\n",
      "Error processing chunk 866: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 867/1053 with 100 users...\n",
      "Error processing chunk 867: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 868/1053 with 100 users...\n",
      "Error processing chunk 868: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 869/1053 with 100 users...\n",
      "Error processing chunk 869: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 870/1053 with 100 users...\n",
      "Error processing chunk 870: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 871/1053 with 100 users...\n",
      "Error processing chunk 871: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 872/1053 with 100 users...\n",
      "Error processing chunk 872: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 873/1053 with 100 users...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing chunk 873: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 874/1053 with 100 users...\n",
      "Error processing chunk 874: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 875/1053 with 100 users...\n",
      "Error processing chunk 875: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 876/1053 with 100 users...\n",
      "Error processing chunk 876: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 877/1053 with 100 users...\n",
      "Error processing chunk 877: [Errno 13] Permission denied: 'final_recommendations.csv'\n",
      "Processing chunk 878/1053 with 100 users...\n",
      "Processing chunk 879/1053 with 100 users...\n",
      "Processing chunk 880/1053 with 100 users...\n",
      "Processing chunk 881/1053 with 100 users...\n",
      "Processing chunk 882/1053 with 100 users...\n",
      "Processing chunk 883/1053 with 100 users...\n",
      "Processing chunk 884/1053 with 100 users...\n",
      "Processing chunk 885/1053 with 100 users...\n",
      "Processing chunk 886/1053 with 100 users...\n",
      "Processing chunk 887/1053 with 100 users...\n",
      "Processing chunk 888/1053 with 100 users...\n",
      "Processing chunk 889/1053 with 100 users...\n",
      "Processing chunk 890/1053 with 100 users...\n",
      "Processing chunk 891/1053 with 100 users...\n",
      "Processing chunk 892/1053 with 100 users...\n",
      "Processing chunk 893/1053 with 100 users...\n",
      "Processing chunk 894/1053 with 100 users...\n",
      "Processing chunk 895/1053 with 100 users...\n",
      "Processing chunk 896/1053 with 100 users...\n",
      "Processing chunk 897/1053 with 100 users...\n",
      "Processing chunk 898/1053 with 100 users...\n",
      "Processing chunk 899/1053 with 100 users...\n",
      "Processing chunk 900/1053 with 100 users...\n",
      "Processing chunk 901/1053 with 100 users...\n",
      "Processing chunk 902/1053 with 100 users...\n",
      "Processing chunk 903/1053 with 100 users...\n",
      "Processing chunk 904/1053 with 100 users...\n",
      "Processing chunk 905/1053 with 100 users...\n",
      "Processing chunk 906/1053 with 100 users...\n",
      "Processing chunk 907/1053 with 100 users...\n",
      "Processing chunk 908/1053 with 100 users...\n",
      "Processing chunk 909/1053 with 100 users...\n",
      "Processing chunk 910/1053 with 100 users...\n",
      "Processing chunk 911/1053 with 100 users...\n",
      "Processing chunk 912/1053 with 100 users...\n",
      "Processing chunk 913/1053 with 100 users...\n",
      "Processing chunk 914/1053 with 100 users...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    os.remove(result_file)\n",
    "\n",
    "start_chunk = 719\n",
    "\n",
    "for i, user_chunk in enumerate(user_chunks[start_chunk:], start=start_chunk):\n",
    "    try:\n",
    "        print(f\"Processing chunk {i + 1}/{len(user_chunks)} with {len(user_chunk)} users...\")\n",
    "        process_user_chunk_and_save(\n",
    "            team_32_data_file=data_file,\n",
    "            team_32_user_map_file=user_mapping_file,\n",
    "            team_32_book_map_file=book_mapping_file,\n",
    "            team_32_titles_file=titles_file,\n",
    "            team_32_result_file=result_file,\n",
    "            team_32_user_group=user_chunk,\n",
    "            team_32_num_similar=10  \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
